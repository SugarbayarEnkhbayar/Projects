---
title: "Clustering of real estate ads for sale in Poland"
author: "Sugarbayar Enkhbayar"
date: "1/13/2023"
output:
html_document:
toc: yes
theme: united
pdf_document:
toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
## Introduction
The clustering model is one of the important models in machine learning. This method is widely used in all fields such as medicine, economics, biology, and chemistry. By clustering users and making the most suitable incentives and activations for each cluster, it is possible to increase profit and revenue. This project will cluster real estate ads for sale in Poland on realting.com. This website is one of the biggest international affiliate sales systems with 20 years of experience. Data is scraped from realting.com with Rvest package.
I also created an interactive dashboard with the Kmeans model using the results of the last model. You can see it at (https://sugarbayar.shinyapps.io/Cluster_KMeans/). Firstly, you have to choose a number of cluster and distance methods. After it, you can see the result of this model as a graphic and table.
## Datasets
### Data collection - web scrape
You need 30 minutes to run the below web scraping code. After doing the web scraping, I saved the results as a df_all.RDS file. We have a total of 137 pages of web data. So I wrote a loop up to 137. Other useful information is written as a comment.
```{r eval=T,warning=F,message=F}
library(rvest)
library(tidyverse)
library(stringr)
library(xml2)
library(knitr)
# copy urls
df_urls=data.frame(urls=character()) # create emply df
for (i in 4:7) { # 137 pages. But in order to show you example, I write 4:7 instead of 1:137
url=paste0('https://realting.com/property-for-sale/poland?page=',i,'&movemap-input=1&slug=property-for-sale&Estate%5Bgeo_id%5D=55&Estate%5Bcurrency%5D=EUR&Estate%5Bx1%5D=3.2080078125000004&Estate%5By1%5D=42.58544425738491&Estate%5Bx2%5D=34.80468750000001&Estate%5By2%5D=59.93300042374631&Estate%5Bzoom%5D=5&sort=-created_at') # create i_th page url
url=read_html(url) # read i_th page link
url_all=html_nodes(url,'a') %>% html_attr('href') # all links on i_th page
url_tail=data.frame(x=url_all[seq(71,129,2)]) # create necessary tail of all links on i_th pag
url_head=data.frame(y=rep('https://realting.com',nrow(url_tail))) # create same head of all links on i_th page
url_full=cbind(url_head,url_tail) # column bind two dfs
url_full$url_long=paste0(url_full$y,url_full$x) # combine head of link and tail of link
add_url=data.frame(urls=url_full$url_long)
df_urls=rbind(df_urls,add_url) # row bind each i_th df
df_urls=unique(df_urls) # unique and check
}
head(df_urls,3)
knitr::opts_chunk$set(echo = TRUE)
# copy price and other data
df_pre=data.frame(y=character(),x=character()) # create empty df. x is text data. y is price data
for (i in 1:5) { # We have about 4000 urls. But in order to show you example, I write 5 instead of nrow(df_urls)
url_read=read_html(df_urls[i,])
url_price=html_nodes(url_read,'.price') %>% html_text() # price data of i_th adv
#url_price=data.frame(y=url_price[1])
url_text=html_nodes(url_read,'.newb-params') %>% html_text2() # text data of i_th adv. use class names
df_add=data.frame(x=url_text,y=url_price[1]) # combine price and text data
df_pre=rbind(df_pre,df_add) # row bind
df_pre<-unique(df_pre) # unique
}
View(df_urls)
